{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im Terminal:\n",
    "# 1) Folder erstellen:\n",
    "# cd C:\\Users\\Engineer\\\"OneDrive - DataCraft GmbH\"\\Dokumente\\DataCraft\\\"4.1. Vorbereitung Praxis\"\n",
    "\n",
    "# 2) Scrapy Projekt anlegen\n",
    "# scrapy startproject Linkedin_Firmen_infos\n",
    "\n",
    "# 3) in Projektverzeichnis navigieren:\n",
    "# cd Linkedin_Firmen_infos\n",
    "\n",
    "# 4) Spider erstellen\n",
    "# scrapy genspider Linkedin_Firmen_Spider linkedin.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code für Crawler\n",
    "#import\n",
    "import json\n",
    "import scrapy\n",
    "\n",
    "\n",
    "# Klassen definieren\n",
    "class Linkedin_Firmen_Spider(scrapy.Spider):\n",
    "    name = \"linkedin_company_profile\"\n",
    "\n",
    "    # hier kommen alle public linkedin pages rein von der cryptofundraising seite, die mit nem andern Crawler abgegriffen werden\n",
    "    public_profile_webpages= [\n",
    "        'https://www.linkedin.com/company/kryptoskatt/about/',\n",
    "        \"\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    def start_requests(self):\n",
    "        \n",
    "        public_profile_index_tracker = 0\n",
    "\n",
    "        start_url = self.public_profile_webpages[public_profile_index_tracker]\n",
    "\n",
    "        yield scrapy.Request(url=start_url, callback=self.parse_response, meta={'public_profile_index_tracker': public_profile_index_tracker})\n",
    "\n",
    "\n",
    "    def parse_response(self, response):\n",
    "        public_profile_index_tracker = response.meta['public_profile_index_tracker']\n",
    "        print('***************')\n",
    "        print('****** Scraping page ' + str(public_profile_index_tracker+1) + ' of ' + str(len(self.public_profile_webpages)))\n",
    "        print('***************')\n",
    "\n",
    "        Firmen_items = {}\n",
    "\n",
    "        # Frage ob Deutsch oder Englisch besser ist\n",
    "        # Vermutlich besser auf Englisch? Oder muss ich es aus Deutsch ändern, weil ich alles auf Deutsch in Linkedin anschaue?        \n",
    "        Firmen_items['name'] = response.css('.top-card-layout__entity-info h1::text').get(default='not-found').strip()\n",
    "        Firmen_items['summary'] = response.css('.top-card-layout__entity-info h4 span::text').get(default='not-found').strip()\n",
    "\n",
    "        try:\n",
    "            ## alle Firmen infos\n",
    "            Firmen_details = response.css('.core-section-container__content .mb-2')\n",
    "\n",
    "            # Branchen\n",
    "            Firma_Branchentyp = Firmen_details[1].css('.text-md::text').getall()\n",
    "            Firmen_items['industry'] = Firma_Branchentyp[1].strip()\n",
    "\n",
    "            # Größe_der_Firma\n",
    "            Größe_der_Firma = Firmen_details[2].css('.text-md::text').getall()\n",
    "            # Wäre jetzt die Frage, ob es dann englisch oder Deutsch sein muss?\n",
    "            Firmen_items['size'] = Größe_der_Firma[1].strip()\n",
    "\n",
    "            #Gründungsdatum\n",
    "            Gründung_der_Firma = Firmen_details[5].css('.text-md::text').getall()\n",
    "            Firmen_items['founded'] = Gründung_der_Firma[1].strip()\n",
    "        except IndexError:\n",
    "            print(\"Error: Firme wurde ausgelassen - Einige Infos fehlen!\")\n",
    "\n",
    "        yield Firmen_items\n",
    "        \n",
    "\n",
    "        public_profile_index_tracker = public_profile_index_tracker + 1\n",
    "\n",
    "        if public_profile_index_tracker <= (len(self.public_profile_webpages)-1):\n",
    "            next_url = self.public_profile_webpages[public_profile_index_tracker]\n",
    "\n",
    "            yield scrapy.Request(url=next_url, callback=self.parse_response, meta={'public_profile_index_tracker': public_profile_index_tracker})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2674254098.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    scrapy crawl public_profile_webpages\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Crawler laufen lassen:\n",
    "# über terminal: scrapy crawl linkedin_company_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nicht geblockt werden => weitere packages notwendig:\n",
    "# pip install scrapeops-scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigene ScrapeOps API key hinzufügen\n",
    "SCRAPEOPS_API_KEY = 'UNSER_API_Key'\n",
    "\n",
    "\n",
    "# Hinzufügen von Erweiterungen im ScrapeOps Extension\n",
    "EXTENSIONS = {\n",
    "'scrapeops_scrapy.extension.ScrapeOpsMonitor': 500, \n",
    "}\n",
    "\n",
    "\n",
    "# Update der Downloader Middlewares\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "'scrapeops_scrapy.middleware.retry.RetryMiddleware': 550,\n",
    "'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
